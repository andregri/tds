\documentclass[../main.tex]{subfiles}

\begin{document}
	\section{Raggiungibilit\'a e controllabilit\'a}
	
	\subsection{Definizione}
		Dato un sistema $ \dot x = A x + B u $:\\
		
		$ \hat x $ \'e uno stato \textbf{raggiungibile} in un tempo $ \hat t $ se $ \exists \hat u(t)\ con\ t \in [0, \hat t] $ tale che se $ x(0^-) = 0 $ e $ u(t) = \hat u(t) $, allora $ x(\hat t) = \hat x $.
		\[ X_R(\hat t)\ \text{sottospazio degli stati raggiungibili in}\ \hat t \]
		
		$ \tilde x $ \'e uno stato \textbf{controllabile} in un tempo $ \tilde t $ se $ \exists \tilde u(t)\ con\ t \in [0, \tilde t] $ tale che se $ x(0^-) = \tilde x $ e $ u(t) = \tilde u(t) $, allora $ x(\tilde t) = 0 $.
		\[ X_C(\hat t)\ \text{sottospazio degli stati controllabili in}\ \hat t \]
		
		\subsection{Propriet\'a per i sistemi LTI a tempo continuo}
		Per i sistemi LTI a tempo continuo valgono le seguenti propriet\'a:
		\begin{itemize}
			\item
				se $ \tilde x $ \'e raggiungibile in $ \tilde t $, se e solo se $ \tilde x $ \'e controllabile.
				\[ X_R(\tilde t) \equiv X_C(\tilde t) \]
				Questo \'e dovuto all'invertibilit\'a della matrice $ e^{A \tilde t} $ (che per la propriet\'a dell'esponenziale si pu\'o calcolare come $ \left( e^{A \tilde t} \right)^{-1} = e^{-A \tilde t} $).
				
			\item
				se $ \tilde x \in X_R(\tilde t) $, allora $ \tilde x \in X_R(\hat t) \forall \hat t > 0 $.
				Lo stesso vale per $ X_C $ per la propriet\'a precedente.
				
				Per i SLTI si parla dunque di $ X_R \equiv X_C $ indipendentemente dal tempo.\\
				Prendiamo:
				\begin{itemize}
					\item
						$ \dot x = A x + B u $ \'e un sistema completamente controllabile in $ \tilde t $;
					\item
						dato uno stato iniziale $ x(0^-) = \hat x $, voglio ottenere che lo stato in $ \tilde t $ sia: $ x(\tilde t) = \tilde x $;
				\end{itemize}
				Dimostriamo questa propriet\'a per un determinato controllo:
				\[ \tilde u(\tau) = B^T e^{A^T (\tilde t - \tau)} \cdot \left[ \int_{0^-}^{\tilde t} e^{A(\tilde t - \epsilon)} B B^T e^{A^T (\tilde t - \epsilon)} d\epsilon \right]^{-1} \cdot \left( \tilde x - e^{A \tilde t} \hat x \right) = B^T e^{A^T (\tilde t - \tau)} \cdot W^{-1}(\tilde t) \cdot \left( \tilde x - e^{A \tilde t} \hat x \right) \]
				Applico l'equazione di Lagrange per calcolare $ x(\tilde t) $:
				\begin{align*}
					x(\tilde t) &= e^{A \tilde t} \hat x + \underbrace{\int_{0^-}^{\tilde t} e^{A(\tilde t - \tau)} B B^T e^{A^T (\tilde t - \tau)}}_{W(\tilde t)} W^{-1}(\tilde t) \left( \tilde x - e^{A \tilde t} \hat x \right) =\\
					&= e^ {A \tilde t} \hat x + \left[ W(\tilde t) W^{-1}(tilde t) \right] \left( \tilde x - e^{A \tilde t} \hat x \right) = \hat x \quad \forall \tilde t > 0
				\end{align*}
				Ho dimostrato che partendo da dove voglio $ \hat x $, arrivo dove voglio $ \tilde x $ in un tempo a piacere, se prendo il giusto controllo $ \tilde u(t) $.
		\end{itemize}
	
		Inoltre valgono le seguenti propriet\'a per $ X_R \equiv X_C $:
		\begin{itemize}
			\item
				$ X_R $ \'e invariante rispetto ad A:
				\[ \tilde x \in X_R \quad\Rightarrow\quad A \tilde x \in X_R \]
				cio\'e se \'e controllabile, allora posso imporre in cui lo stato si muove.
			\item
				$ X_R $ \'e invariante per il sistema:
				\[ x(\hat t) \in X_R \quad\Rightarrow\quad x(t) \in X_R \forall t \geq \hat t \]
				cio\'e se lo stato \'e dentro lo spazio di raggiungibilit\'a, ci rimane per sempre.
			\item
				$ X_R $ \'e un sottospazio lineare: se posso raggiungere due stati $ \tilde x_1 $ e $ \tilde x_2 $, allora posso raggiungere una qualunque combinazione lineare.\\
				Possiamo dimostrarlo con la propriet\'a di sovrapposizione degli effetti:
				dati $ x_A, x_B \in X_R $
				\begin{itemize}
					\item $ x(0^-) = 0 \Rightarrow \exists u_A(\tau) \Rightarrow x(\hat t) = x_A $
					\item $ x(0^-) = 0 \Rightarrow \exists u_B(\tau) \Rightarrow x(\hat t) = x_B $
				\end{itemize}
				\[ u(\tau) = \alpha u_A(\tau) + \beta u_B(\tau) \Rightarrow x(\hat t) = \alpha x_A + \beta x_B \]
				
				Poich\'e si tratta di un sottospazio lineare, $ \tilde x = 0 $ \'e sempre incluso in $ X_R $. Quindi non esiste un sistema con $ X_R $ insieme vuoto, perch\'e l'origine c'\'e sempre.
		\end{itemize}
		
		Definiamo $ X_{NR} = X_R^{\perp} $ come l'insieme di tutti gli stati che non sono raggiungibili. $ X_{NR} $ \textbf{non \'e un sottospazio} perch\'e perch\'e non contiene lo stato $ x = 0 $.
		
	\subsection{Completa controllabilit\'a}
		Supponiamo di avere il seguente sistema:
		\[
			\begin{bmatrix}
				\underline{\dot x_1} \\ \underline{\dot x_2}
			\end{bmatrix} = 
			\begin{bmatrix}
				A_{11} & A_{12}\\
				0 & A_{22}
			\end{bmatrix}
			\begin{bmatrix}
				\underline x_1 \\ \underline x_2
			\end{bmatrix} +
			\begin{bmatrix}
				B_1 \\ 0
			\end{bmatrix} \underline u(t)
		\]
		Dalla seconda equazione vedo che $ \underline x_2 $ da $ \underline u $ direttamente n\'e indirettamente attraverso $ \underline x_1 $. $ \underline x_2 $ non \'e controllabile e quindi il sistema non \'e \textbf{completamente controllabile}.
	
	\subsubsection*{Esempio}
		\[
			\underline{\dot x} =
			\begin{tikzpicture}[baseline=-0.5ex]
			\matrix [matrix of math nodes,left delimiter={[},right delimiter={]}] (m)
			{
				0 & 1 & 1 & 0\\
				0 & 0 & 0 & 19\\
				0 & 0 & 0 & 1\\
				0 & 0 & -1 & 0\\
			};  
			\draw[color=red] (m-3-1.north west) -- (m-3-2.north east) -- (m-4-2.south east) -- (m-4-1.south west) -- (m-3-1.north west);
			\end{tikzpicture} \underline x +
			\begin{tikzpicture}[baseline=-0.5ex]
			\matrix [matrix of math nodes,left delimiter={[},right delimiter={]}] (m)
			{
				0\\
				1\\
				0\\
				0\\
			};  
			\draw[color=red] (m-3-1.north west) -- (m-3-1.north east) -- (m-4-1.south east) -- (m-4-1.south west) -- (m-3-1.north west);
			\end{tikzpicture} \underline u
		\]
		Si nota che la seconda componente dello stato non \'e controllabile:
		\[
			\underline{\dot x_2} =
			\begin{bmatrix}
				0 & 1\\
				-1 & 0
			\end{bmatrix} \underline{x_2}
		\]
		Quindi il sistema non \'e completamente controllabile.\\
		Per esercizio calcoliamo la trasformata della soluzione di questa equazione differenziale:
		\[
			X_2(s) =
			\begin{bmatrix}
				s & -1\\
				1 & s
			\end{bmatrix}^{-1} \underline x(0^-) =
			\begin{bmatrix}
				\frac{s}{s^2+1} & \frac{1}{s^2+1}\\
				\frac{-1}{s^2+1} & \frac{s}{s^2+1}
			\end{bmatrix} \underline x(0^-) =
		\]
		Antitrasformiamo:
		\[
			\underline x_2(t) =
			\begin{bmatrix}
				\cos t & \sin t\\
				-\sin t & \cos t
			\end{bmatrix} 1(t) \underline x(0^-)
		\]
		
	\subsection{Teorema di Cayley-Hamilton}
		Data una matrice quadrata $ A \in \R^{n_x \times n_x} $ e il relativo polinomio caratteristico $ \phi(s) = det(sI-A) $, allora:
		\[ \phi(A) = 0\ \text{matrice nulla} \]
		In generale:
		\[ \phi(s) = s^{n_x} + \phi_{n_x-1} s^{n_x-1} + \dots + \phi_1 s + \phi_0 \]
		Per il teorema di Cayley-Hamilton:
		\begin{align}
			\label{th_ch}
			\phi(A) &= A^{n_x} + \phi_{n_x-1} A^{n_x-1} + \dots + \phi_1 A + \phi_0 I = 0\\
			A^{n_x} &= -\phi_{n_x-1} A^{n_x-1} - \dots - \phi_1 A - \phi_0 I
		\end{align}
		$ A_{n_x} $ \'e combinazione lineare di $ I, A, \dots, A_{n_x-1} $.
		
	\subsubsection{Come calcolare $ A^{n_x+k} \forall k \geq 0 $}
		Per esempio cominciamo da $ k = 1 $:
		\[ A^{n_x+1} = A^{n_x} A = \underbrace{- \phi_{n_x-1} A^{n_x}}_{\text{comb. lineare di}\ I, A, \dots, A_{n_x-1}} \underbrace{- \phi_{n_x-2} A^{n_x-1} - \dots - \phi_1 A^2 - \phi_0 A}_{\text{comb. lineare di}\ I, A, \dots, A_{n_x-1}} \]
		Quindi $ A^{n_x+1} $ \'e combinazione lineare di $ I, A, \dots, A_{n_x-1} $.\\
		In generale $ A^{n_x+k} $ \'e combinazione lineare di $ I, A, \dots, A_{n_x-1} $.
		
	\subsubsection{Com'\'e fatto $ X_R $}
		se ipotizziamo $ \underline x(0^-) $:
		\[ X_R = \left\lbrace \text{sottospazio di tutti i}\ \tilde x\ \text{raggiungibili} \right\rbrace = \left\lbrace \int_{0^-}^{\tilde t} e^{A(\tilde t - \tau)} B \tilde u(\tau) d\tau, \forall \tilde t \right\rbrace \]
		L'integrale pu\'o essere visto come combinazione lineare di infiniti termini tra $ e^{A(\tilde t - \tau)} B $ e $ u(\tau) $:
		\[ X_R = \left\lbrace \text{tutte le possibili combinazioni lineari di tutte le colonne di}\ e^{A(\tilde t - \tau)} B \right\rbrace \]
		Ma abbiamo visto che $ e^{At} = \sum_{k=0}^{\infty} \dfrac{A^k t^k}{k!} $:
		\begin{align*}
			e^{A(\tilde t - \tau)} &= \sum_{k=0}^{\infty} \dfrac{A^k (\tilde t - \tau)^k}{k!} = \sum_{k=0}^{\infty} \dfrac{(\tilde t - \tau)^k}{k!} A^k =\\
			&= I + \alpha_1 A^1 + \alpha_2 A^2 + \dots + \alpha_{n_x-1} A^{n_x-1} + \sum_{k=n_x}^{\infty} \alpha_k A^k
		\end{align*}		
		Abbiamo dimostrato che $ A^k $ \'e combinazione lineare di tutti i termini precedenti $ I, A, \dots, A_{n_x-1} $. Quindi:
		\[ e^{A(\tilde t - \tau)} = \text{combinazione lineare}(I, A, \dots, A_{n_x-1}) \]
		In definitiva l'integrale \'e combinazione lineare delle colonne di $ e^{A(\tilde t - \tau)} B \forall (\tilde t - \tau) $, che equivale a una matrice del tipo:
		\[ \left[ B | AB | \dots | A^{n_x-1} B \right] \]

	\subsection{Teorema di Kalmann di raggiungibilit\'a}
		$ X_R $ \'e combinazione lineare di tutte le colonne di:
		\[ P = \left[ B | AB | \dots | A^{n_x-1} B \right] \]
		Il teorema si pu\'o anche scrivere come:
		\[ P = Immagine\left( \left[ B | AB | \dots | A^{n_x-1} B \right] \right) \]
		
	\subsubsection*{Esempio}
		Alcune volte non tutte le colonne di $ P $ sono necessarie:
		\[
			\underline{\dot x} =
			\begin{bmatrix}
				0 & 1\\
				10 & 5
			\end{bmatrix} \underline x +
			\begin{bmatrix}
				1 & 1\\
				0 & 1
			\end{bmatrix} \underline u
		\]
		Ci sono 2 variabili di controllo $ n_u = 2 $ e 2 variabili di stato $ n_x = 2 $.
		\[
			P = \left[ B | AB \right] =
			\begin{bmatrix}
				1 & 1 & | & \dots\\
				0 & 1 & | & \dots
			\end{bmatrix}
		 \]
		 In questo caso solo con $ B $ ho gi\'a $ n_x = 2 $ colonne linearmente indipendenti: $ \begin{smallmatrix}1\\ 0\end{smallmatrix} $ e $ \begin{smallmatrix}1\\ 1\end{smallmatrix} $. Quindi \'e inutile scrivere le matrici successive di $ P $, perch\'e le loro colonne saranno sicuramente linearmente dipendenti a quelle di $ B $.
		 
		 In conclusione $ dim\left( X_R \right) = 2 $.
		 
	\subsection{Corollario del teorema di Kalmann}
		\begin{itemize}
			\item 
				\[ dim\left( X_R \right) = rank\left( P \right) \]
				La matrice $ P $ ha un numero di colonne e righe pari a:
				\[ P = \left. \left[ \underbrace{\underbrace{B}_{n_u} | \underbrace{AB}_{n_u} | \dots | \underbrace{A^{n_x-1} B}_{n_u}}_{n_u \times n_u \text{colonne}} \right] \right\rbrace\text{$ n_x $ righe} \]
				Quindi il rango di $ P $ pu\'o essere al massimo $ n_x $.
			\item 
				\[ X_R \equiv \R^{n_x} \quad\Leftrightarrow\quad rank\left( P \right) = n_x \quad\Leftrightarrow\quad \text{sistema completamente controllabile} \]
			\item 
				\[ rank\left( P \right) + dim\left( ker\left( P \right) \right) = n_x \]
				dove $ ker $ \'e l'operatore kernel: $ ker\left( P \right) = \underline v | P \underline v = 0 $
			\item 
				Il teorema di Kalmann di raggiungibilit\'a pu\'o essere applicato separatamente su ogni singola variabile di controllo o gruppi di esse.
		\end{itemize}
	
	\subsubsection*{Esempio}
		In questo sistema non tutti i controlli sono necessari per controllare il sistema.
		\[
			\underline{\dot x} =
			\begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix} \underline x +
			\begin{bmatrix}
			1 & 0\\
			0 & 1
			\end{bmatrix} \underline u 
		\]
		\[
			P =
			\begin{bmatrix}
				1 & 0 & | & 0 & 1\\
				0 & 1 & | & 0 & 2
			\end{bmatrix}
		\]
		$ rank\left( P \right) = n_x = 2 $ quindi il sistema \'e completamente controllabile se usiamo entrambi i controlli.
		
		Adesso proviamo ad usare solo $ u_1 $:
		\[
			P_1  = \left[ B_1 | AB_1 \right] =
			\begin{bmatrix}
				1 & | & 0\\
				0 & | & 0
			\end{bmatrix}
		\]
		$ rank\left( P_1 \right) \neq 2 $ quindi il sistema non \'e completamente controllabile con $ u_1 $.
		
		Applichiamo solo $ u_2 $:
		\[
		P_2  = \left[ B_2 | AB_2 \right] =
			\begin{bmatrix}
				0 & | & 1\\
				1 & | & 0
			\end{bmatrix}
		\]
		$ rank\left( P_2 \right) = 2 $ quindi il sistema \'e completamente controllabile con $ u_2 $.
\end{document}